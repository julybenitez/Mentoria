{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Recomendando Papers - Analisis y Curacion de Datos.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMoJPn6zJFsb",
        "colab_type": "text"
      },
      "source": [
        "# Recomendando Papers - Análisis Exploratorio y Curación de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9481Z8gJQn9",
        "colab_type": "text"
      },
      "source": [
        "Arxiv.org participa de la Open Archive Iniciative https://www.openarchives.org/, es decir que provee acceso a toda la metadata de las publicaciones que se suben a su pagina (Leer https://arxiv.org/help/oa).\n",
        "\n",
        " En este practico vamos a aprender como acceder a estos datos, limpiarlos y transformarlos en dataframes como los que utilizaron en la materia Análisis y visualización de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Q-XOGrNJRs",
        "colab_type": "text"
      },
      "source": [
        "## Accediendo a la metadata\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skPv5k8bOSlI",
        "colab_type": "text"
      },
      "source": [
        "La siguiente función nos permite obtener los metadatos de las publicaciones de arxiv.org y devuelve un dataframe de pandas con las siguiente columnas:\n",
        "\n",
        "*  'title'\n",
        "* 'id'\n",
        "* 'abstract'\n",
        "* 'created'\n",
        "* 'categories'\n",
        "* 'doi'\n",
        "* 'journal'\n",
        "\n",
        "Esta función tiene como parámetros **arxiv_set** que indica que set de publicaciones vamos a llamar (math, cs, physics, etc, http://export.arxiv.org/oai2?verb=ListSets)  y **year** que determina el año de las publicaciones. (Tambien puede modificarse facilmente para extraer metadata de fechas determinadas)\n",
        " \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pzoPkFsl3yG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import urllib\n",
        "import datetime\n",
        "#from itertools import ifilter\n",
        "from collections import Counter, defaultdict\n",
        "import xml.etree.ElementTree as ET\n",
        "import urllib.error,urllib.request\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pylab as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import bibtexparser\n",
        "\n",
        "pd.set_option('mode.chained_assignment','warn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF5QkjXylefB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OAI = \"{http://www.openarchives.org/OAI/2.0/}\"\n",
        "ARXIV = \"{http://arxiv.org/OAI/arXiv/}\"\n",
        "\n",
        "def harvest(arxiv_set=\"math\",year=2018):\n",
        "    df = pd.DataFrame(columns=(\"title\", \"abstract\", \"categories\", \"created\", \"id\", \"doi\",'authors','journal'))\n",
        "    base_url = \"http://export.arxiv.org/oai2?verb=ListRecords&\"\n",
        "    url = (base_url +\n",
        "           \"from={}-01-01&until={}-12-31&\".format(year,year) +\n",
        "           \"metadataPrefix=arXiv&set=%s\"%arxiv_set)\n",
        "    \n",
        "    while True:\n",
        "        print(\"fetching\", url)\n",
        "        try:\n",
        "            response = urllib.request.urlopen(url)\n",
        "            \n",
        "        except urllib.error.HTTPError as e:\n",
        "            if e.code == 503:\n",
        "                to = int(e.hdrs.get(\"retry-after\", 30))\n",
        "                print(\"Got 503. Retrying after {0:d} seconds.\".format(to))\n",
        "\n",
        "                time.sleep(to)\n",
        "                continue\n",
        "                \n",
        "            else:\n",
        "                raise\n",
        "            \n",
        "        xml = response.read()\n",
        "\n",
        "        root = ET.fromstring(xml)\n",
        "\n",
        "        for record in root.find(OAI+'ListRecords').findall(OAI+\"record\"):\n",
        "            arxiv_id = record.find(OAI+'header').find(OAI+'identifier')\n",
        "            meta = record.find(OAI+'metadata')\n",
        "            info = meta.find(ARXIV+\"arXiv\")\n",
        "            created = info.find(ARXIV+\"created\").text\n",
        "            created = datetime.datetime.strptime(created, \"%Y-%m-%d\")\n",
        "            categories = info.find(ARXIV+\"categories\").text\n",
        "            \n",
        "            try:\n",
        "              journal=info.find(ARXIV+\"journal-ref\").text\n",
        "            except AttributeError:\n",
        "              journal=None\n",
        "            \n",
        "            authors=info.find(ARXIV+\"authors\")\n",
        "            authors_list=[]\n",
        "            \n",
        "            for author in authors.findall(ARXIV + 'author'):\n",
        "                 try: \n",
        "                  fullname= author.find(ARXIV + 'keyname').text + ' ' + author.find(ARXIV + 'forenames').text\n",
        "                 except:\n",
        "                  try: \n",
        "                   fullname=author.find(ARXIV + 'forenames').text\n",
        "                  except:\n",
        "                   fullname=author.find(ARXIV + 'keyname').text  \n",
        "                 authors_list.append(fullname)\n",
        "\n",
        "                \n",
        "            doi = info.find(ARXIV+\"doi\")\n",
        "            if doi is not None:\n",
        "                doi = doi.text.split()[0]\n",
        "                \n",
        "            contents = {'title': info.find(ARXIV+\"title\").text,\n",
        "                        'id': info.find(ARXIV+\"id\").text,#arxiv_id.text[4:],\n",
        "                        'abstract': info.find(ARXIV+\"abstract\").text.strip(),\n",
        "                        'created': created,\n",
        "                        'categories': \",\".join(categories.split()),\n",
        "                        'doi': doi,\n",
        "                        'authors': \",\".join(authors_list),\n",
        "                        'journal' : journal\n",
        "                        }\n",
        "\n",
        "            df = df.append(contents, ignore_index=True)\n",
        "\n",
        "        # The list of articles returned by the API comes in chunks of\n",
        "        # 1000 articles. The presence of a resumptionToken tells us that\n",
        "        # there is more to be fetched.\n",
        "        token = root.find(OAI+'ListRecords').find(OAI+\"resumptionToken\")\n",
        "        if token is None or token.text is None:\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            url = base_url + \"resumptionToken=%s\"%(token.text)\n",
        "            \n",
        "    return df\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yssbmm7SC63q",
        "colab_type": "text"
      },
      "source": [
        "1 -    Usen la función **harvest()** definida previamente para crear 3 dataframes correspondientes a cada set \"math\", \"cs\" y \"physics\", para el año 2019 y guarden cada uno de ellos como un archivo csv para evitar tener que usar la api de arxiv innecesariamente. (y en los siguientes puntos pueden/deben guardar nuevas versiones del dataset cuando les parezca conveniente para evitar tener que correr procesos nuevamente)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFS5I9kDUdLb",
        "colab_type": "code",
        "outputId": "f2064cba-8335-430c-ced6-2d5e86c76909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "df_math=harvest('math',year=2019)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&from=2019-01-01&until=2019-12-31&metadataPrefix=arXiv&set=math\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|1001\n",
            "Got 503. Retrying after 10 seconds.\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|1001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|2001\n",
            "Got 503. Retrying after 10 seconds.\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|2001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|3001\n",
            "Got 503. Retrying after 10 seconds.\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|3001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|4001\n",
            "Got 503. Retrying after 10 seconds.\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|4001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|5001\n",
            "Got 503. Retrying after 10 seconds.\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|5001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|6001\n",
            "Got 503. Retrying after 10 seconds.\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|6001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|7001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|8001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|9001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|10001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|11001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|12001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|13001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|14001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|15001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|16001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|17001\n",
            "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=3634276|18001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW93L633XTmC",
        "colab_type": "text"
      },
      "source": [
        "2 - A continuación carguen los archivos csv como dataframes de pandas nuevamente y corroboren que los datos esten bien cargados, los tipos de cada columna sean los correctos, que no haya columnas nulas, etc. \n",
        "Además revisen que no haya datos  duplicados (¿podria haber  publicaciones con el mismo titulo? ¿con el mismo abstract? ¿con el mismo id?). Y checkeen que las fechas de cada fila correspondan realmente al año 2019, si existen datos de otros años eliminenlos (¿porque podria pasar esto?¿Cual es la ultima fecha que aparece en el dataset? )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYeW5B8GUVIx",
        "colab_type": "text"
      },
      "source": [
        "3 -  Ahora deben juntar los 3 dataframes en uno solo **df_arxiv_2019** y volver a revisar si hay filas duplicadas. Capitalizar los nombres de las columnas (title --> Title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm5TbjWeXt6f",
        "colab_type": "text"
      },
      "source": [
        "4 - Agregar al dataframe **df_arxiv_2019** columnas nuevas correspondientes a \"Year\", \"Month\" y \"Day\" a partir de la columna \"Created\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_ifgPSiYOGO",
        "colab_type": "text"
      },
      "source": [
        "5 - Crear nuevas columnas 'Math', 'Cs' , 'Physics' con valores 0,1 correspondientes a si la publicación es parte de cada uno de las areas (sets). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viJwH0qaYjAj",
        "colab_type": "text"
      },
      "source": [
        "6 -  Agregar las columnas faltantes\n",
        "* Abstract_Length [int64] : Cantidad de caracteres en el abstract.\n",
        "\n",
        "* Title_Length [int64] : Cantidad de caracteres en el título.\n",
        "\n",
        "* Number_Authors [ int64] : Cantidad de autores.\n",
        "\n",
        "* Number_Fields [int64] : Número de Áreas.\n",
        "\n",
        "* Number_Categories [int64]: Número de Categorias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B7aGi9y70dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}